%!TEX root =  ../main.tex

\begin{abstract}
近年来，卷积神经网络（CNNs）因其高推断精度和强自适应性而被广泛应用于各种领域，例如：计算机视觉、语音识别等。另一方面，移动手机当前已经成为人类日常生活中的随身携带之物，并且每天都产生着大量与人类相关的传感数据。为了让手机更加智能地服务于人类，许多工程项目也尝试着在移动端利用卷积神经网络处理这些传感数据。然而，由于受到当前移动平台的资源限制（内存、计算能力、电池容量等），基于CNN模型的应用在手机移动平台上并不多见。

目前，手机上基于CNN模型的应用绝大部分都是采用“客户端-服务器”模式，但是该模式不仅强依赖于网络性能（如网络稳定性等）而且会导致用户隐私泄露。因此，许多研究学者开始探索如何在移动端离线执行卷积神经网络的前向推断过程。针对这一研究课题，本文提出了一系列优化策略并开发出一套可高能效运行在Android平台的卷积神经网络推断时库。然后，本文利用该推断时库开发了一款生活日志型应用，借以探索从系统层进一步提高该类场景应用运行时能效的策略。论文的主要工作包括：

\begin{enumerate}
\item 利用预训练好的卷积神经网络模型权重在移动端重构网络，并使用OpenCL异构编程框架开发基于手机GPU加速的卷积神经网络推断时库。
\item 基于“剪枝-重训”方法对卷积神经网络模型进行压缩，并在CNN推断时库中引入稀疏矩阵向量乘（SpMV）使得运行时库支持经压缩处理的稀疏CNN模型。
\item 为了充分利用当前以及未来移动设备SoC所提供的异构计算能力，本文提出了一种使用移动平台异构设备处理器并行执行CNN推断的能效优化策略。该策略可根据目标平台所配备异构处理器间的能效差异自适应地寻找一个可高能效并行执行CNN推断的设备处理器组合。
\item 针对基于CNN模型的生活日志型应用，本文详细分析了该类应用的运行时负载特征，并进一步提出了在系统层使用动态电压频率调节技术（DVFS）提高该类应用性能或能效的方法。
\end{enumerate}

本文工作的研究意义主要包括如下三点：

\begin{enumerate}
  \item 设计了一套集成离线模型压缩、异构计算任务分配等功能的移动端CNN推断时库。
  \item 提出了基于异构设备处理器高能效并行执行CNN推断的策略，该策略可在运行时主动对目标平台上的异构处理器能效进行评估。
  \item 探索了从系统层使用DVFS技术优化基于CNN模型移动端智能应用能效的策略。
\end{enumerate}


\keywords{卷积神经网络；移动平台；能效；异构计算；权值压缩；系统层优化}
\end{abstract}

\begin{enabstract}
In recent years, Convolutional Neural Networks (CNNs) have been widely used in various domains, such as computer vision and speech recognition because of their high accuracy and strong self-adaptiveness. On the other hand, mobile phones have become carry-ons to human beings,and generate a large number of sensor data every day. In order to make mobile phones serve people more intelligently, many engineering projects also try to use CNNs to process these sensor data on the mobile phone. However, due to resource limitations (memory, computing power, battery capacity, etc.) of current mobile platforms, CNN-based mobile applications have not become mainstream on mobile platforms.

Currently, most CNN-based mobile applications adopt the client-server computing paradigm. But this paradigm not only depends on network performance (such as network stability) but also leads to privacy leakage. Therefore, many researchers have begun to explore how to perform the inference process of convolutional neural networks directly on the mobile platform. For this research topic, this paper proposes a series of optimization strategies and develops a CNN inference library, which can run on the Android platform in an energy-efficient way. Then, this paper uses the inference library to develop a life-logging application to explore ways to further improve the energy efficiency of such applications. In summary, main contributions of this paper include:

\begin{enumerate}
  \item Reconstructing the convolutional neural network on the mobile phone by pre-trained weights. Using the mobile GPU acceleration by the OpenCL framework to develop a CNN inference library.
  \item Using the pruning-retraining loop to compress CNN models. Introducing Sparse Matrix-Vector Multiplication (SpMV) into the CNN inference library to enable the runtime library to support the compressed sparse CNN model.
  \item To take full advantage of the heterogeneous computing environment provided by current and future mobile device SoCs, this paper proposes an optimization strategy which can use heterogeneous device processors to execute the CNN inference. Based on the energy efficiency difference among heterogeneous processors equipped on the target mobile platform, this strategy can adaptively find an energy-efficient device processor combination to execute the CNN inference in parallel.
  \item This paper analyzes the runtime load of the CNN-based life-logging application in detail and further proposes the method of using Dynamic Voltage and Frequency Scaling (DVFS) to improve the CNN-based application performance or energy efficiency in system level.
\end{enumerate}

The contributions of this paper are as follows:

\begin{enumerate}
  \item Designing a mobile CNN inference library that integrates functions such as model compression and heterogeneous computing task assignment.
  \item Proposing a strategy for parallel execution of CNN inference based on heterogeneous device processors. This strategy can automatically evaluate the energy efficiency of heterogeneous processors on a target platform at runtime.
  \item Exploring strategies of using DVFS to improve the CNN-based smart application energy efficiency in system level.
\end{enumerate}

\enkeywords{CNNs; Mobile platform; Energy efficiency;Weights compression; Heterogeneous computing; System-level optimization}
\end{enabstract} 