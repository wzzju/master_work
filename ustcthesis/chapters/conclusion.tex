\chapter{总结与展望}

\section{本文工作总结}
当今社会，移动手机已然成为人类生活中不可或缺的贴身物品，并且用户对其智能化程度要求也越来越高。为此，许多手机生产商将人工智能技术应用于手机平台，用来为使用者提供更加人性化的服务，如苹果iPhone X、华为 Mate 10系列等。卷积神经网络（CNNs）是人工智能领域中一种成熟的神经网络模型，其已经被应用于手机平台为人类解决生活中遇到的诸多问题，如人脸识别、机器翻译等。为了更好地保障手机用户的隐私并避免网络性能的不稳定性，基于人工智能模型的手机应用愈来愈偏向于使用手机本地端设备处理器完成前向推断过程而非上传到云端服务器执行。本文针对CNN模型于Android手机平台进行离线推断的过程提出了一系列能效优化策略，主要工作包括如下：
\begin{enumerate}
  \item 基于OpenCL异构编程框架开发了一套可于手机GPU上执行CNN前向推断的运行时库。利用该套CNN运行时库，本文于手机移动平台上分别重构了LeNet-5模型和AlexNet模型。实验发现，LeNet-5模型在手机GPU上进行前向推断的执行速度是其在手机CPU上的11倍以上，并且可以节省120多倍的能耗。对于AlexNet这种复杂度较高的模型，其在手机GPU上执行前向推断相较于手机CPU而言性能加速比可达15，而GPU推断能耗也仅为CPU推断能耗的1/30。
  \item 基于“剪枝-重训”的模型压缩方法对卷积神经网络中占存储量主要部分的全连接层权重进行压缩，并在CNN运行时库中引入稀疏矩阵向量乘（SpMV）使能运行时库支持经压缩处理的稀疏CNN模型。实验证明，对网络模型进行压缩不仅可以降低手机内存占用、提高模型加载速度，还可以在一定程度上加速模型于手机端的推断过程。
  \item 
  \item 
\end{enumerate}


\section{未来工作展望}


\cleardoublepage 