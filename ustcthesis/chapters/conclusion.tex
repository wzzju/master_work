\chapter{总结与展望}

\section{本文工作总结}
当今社会，移动手机已然成为人类生活中不可或缺的贴身物品，并且用户对其智能化程度要求也越来越高。为此，许多手机生产商将人工智能技术应用于手机平台，借以为使用者提供更加人性化的服务，如苹果iPhone X、华为 Mate 10系列等。卷积神经网络（CNNs）是人工智能领域中一种成熟的神经网络模型，其已经被应用于手机平台为人类解决生活中遇到的诸多问题，如人脸识别、机器翻译等。为了更好地保障手机用户的隐私并避免网络性能的不稳定，基于人工智能模型的手机应用愈来愈偏向于使用手机本地端设备处理器完成前向推断过程而非上传到云端服务器执行。本文针对CNN模型于Android手机平台进行离线推断的过程提出了一系列能效优化策略，主要工作包括如下：
\begin{enumerate}
  \item 基于OpenCL异构编程框架开发了一套可在手机GPU上执行CNN前向推断的运行时库。利用该套CNN推断时库，本文在手机移动平台上分别重构了LeNet-5模型和AlexNet模型。实验发现，LeNet-5模型在手机GPU上进行前向推断的执行速度是其在手机CPU上的11倍以上，并且可以节省120多倍的能耗。而对于AlexNet这种复杂度较高的模型，其在手机GPU上执行前向推断相较于手机CPU而言性能加速比可达15，而GPU推断能耗也仅为CPU推断能耗的1/30。
  \item 基于“剪枝-重训”的模型压缩方法对卷积神经网络中占存储量主要部分的全连接层权重进行压缩，并在CNN推断时库中引入稀疏矩阵向量乘（SpMV）使得推断时库支持经压缩处理的稀疏CNN模型。实验证明，对网络模型进行压缩不仅可以降低手机内存占用、提高模型加载速度，还可以在一定程度上加速模型于手机端的推断过程。
  \item 结合移动端SoC架构的发展趋势，本文提出了一种基于手机平台异构设备处理器高能效并行执行CNN推断的优化策略。该策略使得CNN推断时库在运行中可自动感知目标平台上不同异构设备处理器的推断能效，并根据这些能效差异自适应找到一个可高能效执行CNN推断的设备处理器组合。实验结果发现，与总是试图使用所有可获得异构设备处理器的贪心策略相比，本文所提出的策略可以获得3.67倍更高的能效且仅仅损失9.7\%的执行速度。
  \item 通过分析智能监控系统Android应用这种基于CNN模型生活日志型APP的系统层负载特征，本文发现系统默认使用的GPU调频策略会产生“乒乓效应”并造成额外的能耗开销，而将GPU频率固定在最大可达频率点CNN推断性能更高且几乎不影响推断能效。因此，本文提出了一种基于应用场景的GPU调频策略，其可自动感知系统上层应用是否为基于CNN模型的生活日志型应用，并根据检测结果及时改变调频行为。
\end{enumerate}


\section{未来工作展望}
本文针对基于CNN模型的手机应用如何在移动平台上高能效执行这一研究问题，提出了一系列能效优化策略，包括结合模型压缩的手机GPU加速推断、使用异构设备处理器并行推断以及根据应用场景的特点进行系统层优化等。然而，为了使CNN模型广泛地应用于商业手机平台，在本文研究内容的基础上仍然存在着许多待优化的工作。

首先，本文实验发现，手机平台的访存速度普遍较低，所以模型体积较大时其推断速度受访存影响较为明显。在以后的研究工作中，针对手机GPU上的CNN前向推断过程，可以使用半浮点型数据代替浮点型数据。虽然这种做法可能会造成些许的推断精度损失，但是其不仅能够降低内存占用和访存能耗还可以在一定程度上提高推断执行速度。另外，若手机GPU存在着本地内存(local memory)，应尽量使用本地内存代替全局内存以加快访存速度。本文研究工作仅压缩了卷积模型中占存储量主要部分的全连接层权重，而未来将会对卷积层权重进行压缩并探索对压缩后的卷积层提供手机端运行支持的方法。

其次，本文提出的基于异构设备处理器高能效并行执行CNN推断的优化策略中没有考虑任务划分所导致的数据通信开销，这将被加入到未来的研究工作计划中。另外，研究中仅按输出节点对单层的计算任务进行划分，在今后的研究工作中将进一步探索更细粒度的计算任务划分方法。

最后，本文对CNN模型推断在系统层进行能效优化的探索是初步的，它为以后的研究工作提供了手机系统层优化CNN推断的思路。针对基于CNN模型的手机应用，未来的工作将探索更加通用、智能的系统功耗管理器。
\cleardoublepage 